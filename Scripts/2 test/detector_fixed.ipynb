{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6e1aa811-d1f8-4353-b55e-2015c1c9ce8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Using columns: URLs='URL', Labels='Label'\n",
      "Extracting features...\n",
      "Processed 336749 URLs\n",
      "Label distribution:\n",
      " Label\n",
      "1    175806\n",
      "0    160943\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training model...\n",
      "Learned 200 new trusted domains from training data\n",
      "[LightGBM] [Info] Number of positive: 140537, number of negative: 128862\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007865 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 977\n",
      "[LightGBM] [Info] Number of data points in the train set: 269399, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Model trained and saved\n",
      "\n",
      "Sample trusted domains: ['gv.at', 'il.us', 'or.jp', 'gov.pk', 'com.my', 'utoronto.ca', 'uk.com', 'utexas.edu', 'edu.cn', 'facebook.com']\n",
      "\n",
      "==================================================\n",
      "PHISHING DETECTOR READY\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter URL to check (or 'quit'):  https://www.facebook.com/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Analysis Result ===\n",
      "URL: https://www.facebook.com/\n",
      "Domain: facebook.com\n",
      "Status: SAFE\n",
      "Confidence: high\n",
      "Probability: 10.00%\n",
      "Key Indicators:\n",
      "- Trusted domain: facebook.com\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter URL to check (or 'quit'):  https://www.msn.com/?ocid=wispr&pc=u477&AR=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Analysis Result ===\n",
      "URL: https://www.msn.com/?ocid=wispr&pc=u477&AR=1\n",
      "Domain: msn.com\n",
      "Status: SAFE\n",
      "Confidence: medium\n",
      "Probability: 42.00%\n",
      "Key Indicators:\n",
      "- Suspicious randomness (entropy: 4.42)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter URL to check (or 'quit'):  http://localhost:8888/notebooks/OneDrive/Bureau/CyberIA/detector_fixed.ipynb?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Analysis Result ===\n",
      "URL: http://localhost:8888/notebooks/OneDrive/Bureau/CyberIA/detector_fixed.ipynb?\n",
      "Domain: localhost:8888\n",
      "Status: PHISHING\n",
      "Confidence: high\n",
      "Probability: 100.00%\n",
      "Key Indicators:\n",
      "- Suspicious randomness (entropy: 4.64)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter URL to check (or 'quit'):  quit\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.parse import urlparse\n",
    "import math\n",
    "import re\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "# ======================\n",
    "# 1. Feature Extraction\n",
    "# ======================\n",
    "def extract_url_features(url):\n",
    "    \"\"\"Extracts phishing detection features from any URL\"\"\"\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc.split(':')[0]\n",
    "        \n",
    "        features = {\n",
    "            'url_length': len(url),\n",
    "            'domain_has_ip': int(bool(re.match(r'^\\d+\\.\\d+\\.\\d+\\.\\d+$', domain))),\n",
    "            'num_special_chars': sum(1 for c in url if c in '/:?&%=.-_~@'),\n",
    "            'num_digits': sum(c.isdigit() for c in url),\n",
    "            'domain_length': len(domain),\n",
    "            'subdomain_length': len(domain.split('.')[0]),\n",
    "            'num_subdomains': len(domain.split('.')) - 1,\n",
    "            'is_common_tld': int(domain.endswith(('.com', '.org', '.net', '.gov', '.edu', '.io'))),\n",
    "            'typosquatting': int(any(t in domain.lower() for t in ['paypa1', 'g00gle', 'amaz0n', 'faceb00k', 'y0utube'])),\n",
    "            'has_banking_kw': int(any(kw in url.lower() for kw in ['login', 'bank', 'account', 'secure', 'verify'])),\n",
    "            'has_hex': int(bool(re.search(r'%[0-9a-fA-F]{2}', url))),\n",
    "            'has_at_symbol': int('@' in url),\n",
    "            'uses_https': int(parsed.scheme == 'https'),\n",
    "            'path_depth': parsed.path.count('/'),\n",
    "            'entropy': -sum((url.count(c)/len(url)) * math.log2(url.count(c)/len(url)) \n",
    "                       for c in set(url) if url.count(c) > 0),\n",
    "            'vowel_ratio': sum(1 for c in domain if c.lower() in 'aeiou') / len(domain) if domain else 0,\n",
    "            'consecutive_chars': int(bool(re.search(r'([a-zA-Z])\\1{2}', domain))),\n",
    "        }\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting features: {e}\")\n",
    "        return None\n",
    "\n",
    "# ===========================\n",
    "# 2. Phishing Detector Class\n",
    "# ===========================\n",
    "class PhishingDetector:\n",
    "    def __init__(self, model_path=None):\n",
    "        self.model = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', LGBMClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=7,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "        # Pre-trusted domains (can be expanded)\n",
    "        self.trusted_domains = {\n",
    "            'google.com', 'youtube.com', 'facebook.com', 'github.com',\n",
    "            'amazon.com', 'wikipedia.org', 'microsoft.com', 'apple.com',\n",
    "            'twitter.com', 'linkedin.com', 'netflix.com', 'spotify.com'\n",
    "        }\n",
    "        self.domain_reputation = defaultdict(int)\n",
    "        \n",
    "        if model_path:\n",
    "            try:\n",
    "                saved_data = joblib.load(model_path)\n",
    "                self.model = saved_data['model']\n",
    "                self.trusted_domains.update(saved_data.get('trusted_domains', []))\n",
    "                print(\"Loaded pre-trained model with dynamic domain knowledge\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load model: {e}\")\n",
    "\n",
    "    def _extract_base_domain(self, url):\n",
    "        \"\"\"Extracts base domain (e.g. 'google.com' from 'mail.google.com')\"\"\"\n",
    "        try:\n",
    "            domain = urlparse(url).netloc.lower()\n",
    "            if domain.startswith('www.'):\n",
    "                domain = domain[4:]\n",
    "            parts = domain.split('.')\n",
    "            if len(parts) > 2:\n",
    "                return f\"{parts[-2]}.{parts[-1]}\"\n",
    "            return domain\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    def is_trusted_domain(self, url):\n",
    "        \"\"\"Check if domain is in trusted list\"\"\"\n",
    "        domain = self._extract_base_domain(url)\n",
    "        return domain in self.trusted_domains\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"Train model and learn trusted domains\"\"\"\n",
    "        # Extract domains from training data\n",
    "        domains = [self._extract_base_domain(url) for url in X.index]\n",
    "        legit_domains = [d for d, label in zip(domains, y) if label == 0]\n",
    "        \n",
    "        # Calculate domain frequencies\n",
    "        domain_counts = pd.Series(legit_domains).value_counts()\n",
    "        \n",
    "        # Automatically identify trusted domains (top 200 legitimate domains)\n",
    "        new_trusted = set(domain_counts.head(200).index)\n",
    "        self.trusted_domains.update(new_trusted)\n",
    "        print(f\"Learned {len(new_trusted)} new trusted domains from training data\")\n",
    "        \n",
    "        # Train the model\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"Save model with learned domain knowledge\"\"\"\n",
    "        joblib.dump({\n",
    "            'model': self.model,\n",
    "            'trusted_domains': list(self.trusted_domains)\n",
    "        }, path)\n",
    "\n",
    "    def predict(self, url):\n",
    "        \"\"\"Predict with dynamic domain analysis\"\"\"\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = 'https://' + url\n",
    "            \n",
    "        domain = self._extract_base_domain(url)\n",
    "        \n",
    "        # Dynamic trusted domain check\n",
    "        if self.is_trusted_domain(url):\n",
    "            return {\n",
    "                'url': url,\n",
    "                'is_phishing': False,\n",
    "                'confidence': 'high',\n",
    "                'probability': max(0.01, 0.1 - (self.domain_reputation.get(domain, 0) * 0.001)),\n",
    "                'domain': domain,\n",
    "                'indicators': [f'Trusted domain: {domain}']\n",
    "            }\n",
    "\n",
    "        features = extract_url_features(url)\n",
    "        if not features:\n",
    "            return {\n",
    "                'url': url,\n",
    "                'is_phishing': None,\n",
    "                'error': 'Could not extract features',\n",
    "                'confidence': 'low'\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            features_df = pd.DataFrame([features])\n",
    "            proba = self.model.predict_proba(features_df)[0][1]\n",
    "            \n",
    "            # Dynamic probability adjustments\n",
    "            if features['uses_https']:\n",
    "                proba *= 0.7\n",
    "            if features['is_common_tld']:\n",
    "                proba *= 0.6\n",
    "            if domain.endswith(('.gov', '.edu', '.mil')):\n",
    "                proba *= 0.3\n",
    "            \n",
    "            # Final verdict (threshold = 0.75)\n",
    "            is_phishing = proba > 0.75\n",
    "            \n",
    "            indicators = []\n",
    "            if features['domain_has_ip']:\n",
    "                indicators.append(\"Uses IP address (risky)\")\n",
    "            if features['typosquatting']:\n",
    "                indicators.append(\"Typosquatting detected\")\n",
    "            if features['entropy'] > 4.0 and not self.is_trusted_domain(url):\n",
    "                indicators.append(f\"Suspicious randomness (entropy: {features['entropy']:.2f})\")\n",
    "            if features['has_banking_kw'] and not self.is_trusted_domain(url):\n",
    "                indicators.append(\"Contains sensitive keywords\")\n",
    "            \n",
    "            return {\n",
    "                'url': url,\n",
    "                'is_phishing': bool(is_phishing),\n",
    "                'probability': float(proba),\n",
    "                'confidence': 'high' if (proba > 0.9 or proba < 0.2) else 'medium',\n",
    "                'domain': domain,\n",
    "                'indicators': indicators if indicators else ['No strong indicators']\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'url': url,\n",
    "                'is_phishing': None,\n",
    "                'error': str(e),\n",
    "                'confidence': 'low'\n",
    "            }\n",
    "\n",
    "# =================\n",
    "# 3. Main Execution\n",
    "# =================\n",
    "if __name__ == \"__main__\":\n",
    "    detector = PhishingDetector()\n",
    "    \n",
    "    try:\n",
    "        print(\"Loading dataset...\")\n",
    "        data = pd.read_csv('C:/Users/msi/OneDrive/Bureau/CyberIA/StealthPhisher2025.csv')\n",
    "        \n",
    "        # Auto-detect columns\n",
    "        url_col = next((col for col in data.columns if 'url' in col.lower()), 'URL')\n",
    "        label_col = next((col for col in data.columns if col.lower() in ['label', 'phishing']), 'Label')\n",
    "        \n",
    "        print(f\"Using columns: URLs='{url_col}', Labels='{label_col}'\")\n",
    "        \n",
    "        # Convert labels\n",
    "        label_mapping = {\n",
    "            'legitimate': 0, 'phishing': 1, 'safe': 0, 'malicious': 1,\n",
    "            '0': 0, '1': 1, 'false': 0, 'true': 1\n",
    "        }\n",
    "        y = data[label_col].astype(str).str.lower().map(label_mapping)\n",
    "        \n",
    "        if y.isna().any():\n",
    "            invalid_labels = data[label_col][y.isna()].unique()\n",
    "            raise ValueError(f\"Found unmapped labels: {invalid_labels}\")\n",
    "        \n",
    "        # Extract features\n",
    "        print(\"Extracting features...\")\n",
    "        features = []\n",
    "        valid_indices = []\n",
    "        url_index = []\n",
    "        \n",
    "        for i, url in enumerate(data[url_col]):\n",
    "            feat = extract_url_features(str(url))\n",
    "            if feat:\n",
    "                features.append(feat)\n",
    "                valid_indices.append(i)\n",
    "                url_index.append(url)\n",
    "        \n",
    "        X = pd.DataFrame(features, index=url_index)\n",
    "        y = y.iloc[valid_indices].astype(int)\n",
    "        \n",
    "        print(f\"Processed {len(X)} URLs\")\n",
    "        print(\"Label distribution:\\n\", y.value_counts())\n",
    "        \n",
    "        # Train model\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        print(\"\\nTraining model...\")\n",
    "        detector.train(X_train, y_train)\n",
    "        detector.save_model('phishing_model.pkl')\n",
    "        print(\"Model trained and saved\")\n",
    "        \n",
    "        # Show some learned domains\n",
    "        print(\"\\nSample trusted domains:\", list(detector.trusted_domains)[:10])\n",
    "        \n",
    "        # Interactive testing\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"PHISHING DETECTOR READY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        while True:\n",
    "            url = input(\"\\nEnter URL to check (or 'quit'): \").strip()\n",
    "            if url.lower() == 'quit':\n",
    "                break\n",
    "                \n",
    "            result = detector.predict(url)\n",
    "            print(\"\\n=== Analysis Result ===\")\n",
    "            print(f\"URL: {result['url']}\")\n",
    "            print(f\"Domain: {result.get('domain', 'N/A')}\")\n",
    "            print(\"Status:\", \"PHISHING\" if result.get('is_phishing') else \"SAFE\")\n",
    "            print(f\"Confidence: {result.get('confidence', 'unknown')}\")\n",
    "            if 'probability' in result:\n",
    "                print(f\"Probability: {result['probability']:.2%}\")\n",
    "            print(\"Key Indicators:\")\n",
    "            for indicator in result.get('indicators', ['No strong indicators']):\n",
    "                print(f\"- {indicator}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: {str(e)}\")\n",
    "        print(\"\\nTroubleshooting Guide:\")\n",
    "        print(\"1. Verify CSV file exists and is accessible\")\n",
    "        print(\"2. Check column headers match expected format\")\n",
    "        print(\"3. Ensure URLs are properly formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1dc8692c-8a9b-4486-9d1c-3076dbfec5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download ready: phishing_model.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive('phishing_model', 'zip', '.', 'phishing_model.pkl')\n",
    "print(\"Download ready: phishing_model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14bbabca-85dc-40ab-812e-2429ace4fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this line in the main execution section:\n",
    "detector.save_model('phishing_model.pkl')  # Old version\n",
    "# To this:\n",
    "detector.save_model('PH.pkl')  # New version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d922d97c-c7ac-4b81-8854-48948722b160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
